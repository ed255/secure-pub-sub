\section{Evaluation}
\label{sec:evaluation}

\subsection{Setup}

We have evaluated our implementation in an environment consisting on two
computers located in the same Gigabit Ethernet network.  We run the Broker,
Third Party and one Subscriber on the first computer, and all the Publishers in
the second computer.  We perform all the analysis on the performance of several
operations in the Broker and Third Party, which run together in a computer
running Linux with an Intel Core i7-6600U CPU with 16GB of RAM.  In order to
avoid isolate the performance influence of the Broker and the Third Party, we
have serialized the garbling and evaluation of the garbled circuit operations,
mimicking the situation in which the Broker and Third Party are run on
different servers.

Every measured time shown in this section (either in numerical form or in a
plot) has been obtained by taking the mean out of 5 repetitions in the same
configuration.  In the plots, a confidence interval of 95\% for the mean result
(out of the 5 samples) is shown in gray.

In all our measured times, sending includes the marshaling and unmarshaling of
the garbled circuit and associated data structures necessary for transmitting
it from the Third Party to the Broker via RPC.

\subsection{Microbenchmarks}

We have selected 5 numerical operations of varying complexity (\emph{summation,
multiplicatory, mean, variance, minimum/maximum}) to evaluate the cost of the
different parts of our implementation.  We securely evaluate these functions
over the values (encoded as 32 bit fixed point numbers) received from a
variable number of Publishers.

% Plots
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{plots/garble_loglog.png}
        \caption{Garble}
        \label{fig:micro-garble-time}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{plots/eval_loglog.png}
        \caption{Evaluate}
        \label{fig:micro-eval-time}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{plots/send_loglog.png}
        \caption{Send}
        \label{fig:micro-send-time}
    \end{subfigure}
    \caption{Mean time required for garbling, evaluating and sending the
    garbled circuit to the Broker for each function in the microbenchmark.
    Sending includes the marshaling and unmarshaling of the garbled circuit and
    associated data structures.}
    \label{fig:micro-times}
\end{figure*}


\begin{figure}
  \includegraphics[width=0.45\textwidth]{plots/nonxor_gates_log.png}
  \caption{Non-XOR gates count per function used in the microbenchmarks.}
  \label{micro-nonxor}
\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{plots/enc_dec_inputs.png}
  \caption{Time spent garbling and evaluating the identity input gates.}
  \label{micro-inputs}
\end{figure}

%\begin{figure}
%  \includegraphics[width=0.45\textwidth]{plots/sum_circuit_2017-05-15.png}
%  \caption{Time spent garbling the circuit, sending the result to the Broker and evaluating it.}
%\end{figure}

%\begin{figure}
%  \includegraphics[width=0.45\textwidth]{plots/sum_circuit_onepub_2017-05-13.png}
%  \caption{Time spent garbling the circuit, sending the result to the Broker
%  and evaluating it removing the overhead of handling the Publishers
%  connections.}
%\end{figure}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{plots/size_log.png}
  \caption{Size of the garbled circuit and the associated date required by the
  Broker to evaluate it.}
  \label{micro-sizes}
\end{figure}

We can see that the time spent encrypting and decrypting the inputs (that is,
garbling and evaluating the input identity gates) makes a significant influence
in the mean, min/max and sum microbenchmark, being comparable in magnitude to
the evaluation and garbling time (notice that for sum, the input
encryption/decryption time is even higher than the garbling/evaluation time).
We attribute this behavior to the fact that this part of the protocol is
implemented in go instead of C like the garbling and evaluation.

\subsection{Applications}

% TODO: 1. Turonet, wireless propagation constant, linear regression
\paragraph{Linear regression}

% One dimensional linear regression

% Questions: How many data points to use?

% Formula:
In this application we are interested in learning the linear model of two data
streams generated by Publishers.  In particular, we perform a linear regression
so that we can model one variable (comming from one stream) as a linear
combination of another variable (comming from another stream): $y = ax + b$.
To estimate the parameters of the one dimensional linear model we use the
ordinary least squares technique, which gives us the following closed-form
formula:

$\begin{pmatrix} b \\ c \end{pmatrix} =
\left( \displaystyle\sum_{i=1}^n \begin{pmatrix} 1 \\ x_i \end{pmatrix}
  \begin{pmatrix} 1 & x_i\end{pmatrix}\right)^{-1}
\left( \displaystyle\sum_{i=1}^n y_i \begin{pmatrix} 1 \\ x_i \end{pmatrix}\right)$

Evaluating the formula requires an inversion of a $2 x 2$ matrix, which we perform
by following the analytic solution.

We evaluate the cost of computing the one-dimensional linear regression over
two streams with a varying number of samples.

In this application's scenario, we assume that each sensor would be an
individual Publisher that sends readings periodically.  The Broker would
accumulates streams of published values from each sensor, and when requested,
the it would samples pairs of those streams at random over a specific period of
time to perform a correlation analysis between the two.  For our experiments we
vary the number of samples and simulate the application by supplying the Broker
with the given number of samples in a batch.


% TODO: 2. Environmental Berkeley indor sensing data, correlation
\paragraph{Correlation}

% The dataset provides 2.3 million readings collected from sensors with the
% following values: temperature (degree Celcius), humidity (0-100%), light
% (Lux: 0-100000), identified by sensor ID.

% Ideas: correlation temperature~humidity, temperature~light.

Similarly to the linear regression application, in this application we have the
same scenario with sensors acting as Publishers sending a periodic stream of
reads.  Again, for our experiments we simplify the setting by supplying the
Broker with a specific number of samples in a batch.  We evaluate the cost of
the operation by varying the number of samples as before.

% Formula:
The correlation is a measure of statistical relationship among pairs of
variables, which higher value representing higher dependence.  Following the
Pearson's product-moment coefficient, we can measure the dependence between
samples of two variables (in our case, sensor readings) using the following
closed-form formula:

$r_{xy} = \frac{\displaystyle\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}
{\sqrt{\displaystyle\sum_{i=1}^n (x_i - \bar{x})^n (y_i - \bar{y})^n}}$

To improve efficiency, we evaluate the squared correlation ($r_{xy}^2$) to avoid
computing the square root in the function circuit, which would be an expensive
operation.

\begin{figure}
  \includegraphics[width=0.45\textwidth]{plots/stream.png}
  \caption{Mean time required for garbling, evaluating and sending the garbled
  circuit to the Broker for computing the correlation and linear regression of
  data streams.  Garbling and evaluating
  includes the time to garble and evaluate the input identity.}
  \label{stream-times}
\end{figure}

% TODO: 3. LAX parking lot dataset, statistics
\paragraph{Continuous statistics of incoming data}

% https://data.lacity.org/dataset/Los-Angeles-International-Airport-LAX-Parking-Lots/dik5-hwp6
% The dataset provides at a frequency of every 5 minutes and for every one of
% the 9 parking lots: total, occupied and free parking spaces.
% That's 288 values per day per parking lot.

% Privacy: Avoid revealing parking lot data at fine granularity, only reveal
% statistics of the data accumulated throughout the day.  Don't reveal data of
% individual parking lots, only combined data (except for the lot with
% more/less free spots)

Setting: We have 9 Publishers, one for each lot, which send data (current
number of free spots, current number of used spots) every 5 minutes.  The
Broker accumulates the data of a day and computes the statistics.  This
happens once per day.

% Daily statistics (used spots): mean, max, min, var of the number of cars in all the lots combined.
% Daily statistics (free spots): The marking lot which had more free spots, and the one
% that had less on average during the day.

% Time garble and time eval includes the time spend encrypting and decrypting
% the input values.

\begin{figure}
    \begin{tabular}{l*{3}{r}r}
    \textbf{Statistic}  & \textbf{Garble} & \textbf{Send} & \textbf{Evaluate} & \textbf{Size} \\
    \hline
    Mean       & 199.8 ms & 226.0  ms & 98.7  ms & 44.7 MB \\
    Max/Min    & 163.3 ms & 172.4  ms & 84.9  ms & 32.0 MB \\
    Variance   & 500.7 ms & 1055.6 ms & 236.1 ms & 222.1 MB \\
    \hline
    Rank free  & 121.3 ms & 85.0 ms & 67.5 ms & 16.6 MB \\
    \end{tabular}
    \caption{Mean time required for the different steps of the protocol to
    evaluate different statistical measures of the parking lot dataset.
    Garbling and evaluating includes the time to garble and evaluate the input
    identity.}
    \label{stats-times}
\end{figure}

% "mean time_garble = 199.79 time_send = 226.06 time_eval = 98.73 size = 45768.73"
% "max  time_garble = 163.28 time_send = 172.38 time_eval = 84.93 size = 32744.84"
% "min  time_garble = 164.40 time_send = 165.77 time_eval = 84.14 size = 32744.84"
% "var  time_garble = 500.66 time_send = 1055.63 time_eval = 236.08 size = 227416.17"

% "max_free time_garble = 121.27 time_send = 85.04 time_eval = 67.55 size = 17059.81"
% "min_free time_garble = 120.98 time_send = 85.58 time_eval = 67.81 size = 17059.81"

% TODO: 4. Road Volume sensor traffic, evaluation of the expected time to follow a path
%\paragraph{Estimation of time required to follow a path with traffic}

% http://rtmap.metro.net/ <- Not working (2017-05-15).

% TODO: 5. Smart bill, monthly electricity bill with different cost per hour of the day / threshold.

% ???

% TODO: Discussion: bottlenecks.

As expected, the time spent sending the garbled circuit from the Third Party to
the Broker is the most expensive part of the protocol, and thus is the current
bottleneck.  This means that the quality and bandwidth of the network
connection between the Broker and the Third Party is of critical importance.

The garbling and evaluation of the identity gates could be optimized by
incorporating the operations in the C code of libgarble.  Even though the
golang implementation of the AES encryption and decryption functions use the
Intel AES-NI hardware instructions, conversions from byte vectors to AES blocks
and vice versa slow down the operation.  On the other hand, libgarble works
natively with 128 byte data types (the size of an AES block) by using the Intel
SSE2 extensions, thus not requiring any conversion during
encryption/decryption.
